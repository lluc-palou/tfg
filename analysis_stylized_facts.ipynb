{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "871ac5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, lead, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f9ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes Spark session with MongoDB connector\n",
    "jar_files_path = \"file:///C:/Users/llucp/spark_jars/\"\n",
    "\n",
    "jar_files = [\n",
    "    \"mongo-spark-connector_2.12-10.1.1.jar\",\n",
    "    \"mongodb-driver-core-4.10.1.jar\",\n",
    "    \"mongodb-driver-sync-4.10.1.jar\",\n",
    "    \"bson-4.10.1.jar\"\n",
    "]\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"tfg\"\n",
    "FORMATTING_TRADE = \"formatting_trade\"\n",
    "FORMATTING_LOB = \"formatting_lob\"\n",
    "EXPLOITATION = \"exploitation\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FormattedZone\")\n",
    "    .config(\"spark.jars\", \",\".join([jar_files_path + jar for jar in jar_files]))\n",
    "    .config(\"spark.mongodb.read.connection.uri\", MONGO_URI)\n",
    "    .config(\"spark.mongodb.write.connection.uri\", MONGO_URI)\n",
    "    .config(\"spark.mongodb.read.database\", DB_NAME)\n",
    "    .config(\"spark.mongodb.write.database\", DB_NAME)\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79b0c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trade() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads formatting zone trade data and returns it as a dataframe.\n",
    "    \"\"\"\n",
    "    trades = (\n",
    "        spark.read.format(\"mongodb\")\n",
    "        .option(\"database\", DB_NAME)\n",
    "        .option(\"collection\", FORMATTING_TRADE)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    return trades\n",
    "\n",
    "trades = load_trade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c54a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_forward_log_returns(df: DataFrame, lag: int, N: int) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates forward log-returns of last traded price over N periods, accounting for a decision lag.\n",
    "    \"\"\"\n",
    "    w = Window.orderBy(\"timestamp\")\n",
    "    base = lead(col(\"last_traded_price\"), lag).over(w)\n",
    "    future = lead(col(\"last_traded_price\"), lag + N).over(w)\n",
    "\n",
    "    return df.withColumn(f\"fwd_log_return_{N}\", log(future) - log(base))\n",
    "\n",
    "# Given a list of forward log-return horizons, calculates and adds them to the dataframe\n",
    "fwd_log_return_horizons = [2, 3, 4, 5, 10, 20, 40, 60, 120, 240]\n",
    "lag = 1  # Decision lag of 1 period\n",
    "\n",
    "for N in fwd_log_return_horizons:\n",
    "    trades = calculate_forward_log_returns(trades, lag, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0716a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "173ea963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects only the forward log-return columns and converts to Pandas dataframe for analysis\n",
    "cols = [f\"fwd_log_return_{N}\" for N in fwd_log_return_horizons]\n",
    "fwd_log_returns = trades.select(*cols).dropna().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd1bcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fwd_log_return_horizon(colname: str) -> int:\n",
    "    \"\"\"\n",
    "    Given the column name of a forward log-return, extracts and returns the horizon (N).\n",
    "    \"\"\"\n",
    "    m = re.search(r\"(\\d+)$\", colname)\n",
    "    \n",
    "    return int(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4a05057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stride_fwd_log_returns(df: pd.DataFrame, cols: list[str]) -> dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Returns dict of stride-sampled series per forward log-return.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    for c in cols:\n",
    "        N = extract_fwd_log_return_horizon(c)\n",
    "        out[c] = df[c].iloc[::N].dropna().reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "fwd_log_returns_dict = apply_stride_fwd_log_returns(fwd_log_returns, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fcba6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c42caafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional tests / models\n",
    "RUNS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from statsmodels.sandbox.stats.runs import runstest_1samp as runs_test\n",
    "    RUNS_AVAILABLE = True\n",
    "\n",
    "except Exception:\n",
    "    runs_test = None\n",
    "\n",
    "BDS_AVAILABLE = False\n",
    "bds_impl = None\n",
    "\n",
    "try:\n",
    "    # Prefer arch.unitroot implementation\n",
    "    from arch.unitroot import BDS as BDS_cls\n",
    "    BDS_AVAILABLE = True\n",
    "    bds_impl = \"arch\"\n",
    "\n",
    "except Exception:\n",
    "    try:\n",
    "        from statsmodels.sandbox.stats.runs import bds as bds_sm\n",
    "        BDS_AVAILABLE = True\n",
    "        bds_impl = \"sm\"\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "MS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "    MS_AVAILABLE = True\n",
    "    \n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d7e8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def _safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _series_clean(s: pd.Series) -> pd.Series:\n",
    "    return pd.Series(s).dropna().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1706a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_series(s: pd.Series) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "    out = {\n",
    "        \"n\": int(s.size),\n",
    "        \"mean\": _safe_float(s.mean()),\n",
    "        \"std\": _safe_float(s.std(ddof=1)),\n",
    "        \"skew\": _safe_float(s.skew()),\n",
    "        \"kurt\": _safe_float(s.kurt()),\n",
    "        \"min\": _safe_float(s.min()),\n",
    "        \"max\": _safe_float(s.max()),\n",
    "        \"median\": _safe_float(s.median()),\n",
    "    }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27a519b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normality_tests(s: pd.Series, shapiro_max_n: int = 5000) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "    out = {}\n",
    "\n",
    "    # Shapiro–Wilk (only up to ~5k recommended)\n",
    "    try:\n",
    "        if s.size <= shapiro_max_n:\n",
    "            sh_stat, sh_p = stats.shapiro(s.values)\n",
    "            out[\"shapiro_stat\"] = _safe_float(sh_stat)\n",
    "            out[\"shapiro_p\"] = _safe_float(sh_p)\n",
    "\n",
    "        else:\n",
    "            out[\"shapiro_skipped\"] = f\"n={s.size} > {shapiro_max_n}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        out[\"shapiro_error\"] = str(e)\n",
    "\n",
    "    # Jarque–Bera\n",
    "    try:\n",
    "        jb_stat, jb_p = stats.jarque_bera(s.values)\n",
    "        out[\"jb_stat\"] = _safe_float(jb_stat)\n",
    "        out[\"jb_p\"] = _safe_float(jb_p)\n",
    "\n",
    "    except Exception as e:\n",
    "        out[\"jb_error\"] = str(e)\n",
    "\n",
    "    # Anderson–Darling for normal (gives critical values)\n",
    "    try:\n",
    "        ad_res = stats.anderson(s.values, dist=\"norm\")\n",
    "        out[\"anderson_stat\"] = _safe_float(ad_res.statistic)\n",
    "        out[\"anderson_crit_vals\"] = [ _safe_float(x) for x in ad_res.critical_values.tolist() ]\n",
    "        out[\"anderson_signif\"] = [ _safe_float(x) for x in ad_res.significance_level.tolist() ]\n",
    "        \n",
    "    except Exception as e:\n",
    "        out[\"anderson_error\"] = str(e)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0387b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr_tests(s: pd.Series, lb_lags: List[int] = [10, 20, 40]) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "    out = {\"ljung_box_returns\": {}, \"ljung_box_squared\": {}}\n",
    "\n",
    "    for L in lb_lags:\n",
    "        try:\n",
    "            lb_r = acorr_ljungbox(s, lags=[L], return_df=True)\n",
    "            p_r = lb_r[\"lb_pvalue\"].iloc[-1]\n",
    "            out[\"ljung_box_returns\"][f\"lags_{L}\"] = _safe_float(p_r)\n",
    "\n",
    "        except Exception as e:\n",
    "            out[\"ljung_box_returns\"][f\"lags_{L}\"] = {\"error\": str(e)}\n",
    "\n",
    "        try:\n",
    "            lb_r2 = acorr_ljungbox(s**2, lags=[L], return_df=True)\n",
    "            p_r2 = lb_r2[\"lb_pvalue\"].iloc[-1]\n",
    "            out[\"ljung_box_squared\"][f\"lags_{L}\"] = _safe_float(p_r2)\n",
    "\n",
    "        except Exception as e:\n",
    "            out[\"ljung_box_squared\"][f\"lags_{L}\"] = {\"error\": str(e)}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a617128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch_lm_tests(s: pd.Series, lags: List[int] = [5, 10, 20]) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "    out = {}\n",
    "\n",
    "    for L in lags:\n",
    "        try:\n",
    "            lm_stat, lm_p, f_stat, f_p = het_arch(s, nlags=L)\n",
    "            out[f\"lags_{L}\"] = {\"LM_p\": _safe_float(lm_p), \"F_p\": _safe_float(f_p)}\n",
    "\n",
    "        except Exception as e:\n",
    "            out[f\"lags_{L}\"] = {\"error\": str(e)}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aa2a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_tests(s: pd.Series) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "    out = {}\n",
    "\n",
    "    try:\n",
    "        adf_stat, adf_p, *_ = adfuller(s, autolag=\"AIC\")\n",
    "        out[\"adf_stat\"] = _safe_float(adf_stat)\n",
    "        out[\"adf_p\"] = _safe_float(adf_p)\n",
    "\n",
    "    except Exception as e:\n",
    "        out[\"adf_error\"] = str(e)\n",
    "\n",
    "    try:\n",
    "        kpss_stat, kpss_p, *_ = kpss(s, regression=\"c\", nlags=\"auto\")\n",
    "        out[\"kpss_stat\"] = _safe_float(kpss_stat)\n",
    "        out[\"kpss_p\"] = _safe_float(kpss_p)\n",
    "        \n",
    "    except Exception as e:\n",
    "        out[\"kpss_error\"] = str(e)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2863de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bds_test(s: pd.Series, max_dim: int = 5) -> Dict[str, Any]:\n",
    "    s = _series_clean(s)\n",
    "\n",
    "    if not BDS_AVAILABLE:\n",
    "        return {\"available\": False, \"error\": \"BDS not available (need arch or statsmodels sandbox).\"}\n",
    "    \n",
    "    out = {\"available\": True, \"impl\": bds_impl, \"results\": {}}\n",
    "\n",
    "    if bds_impl == \"arch\":\n",
    "        try:\n",
    "            res = BDS_cls(s, max_dim=max_dim)\n",
    "\n",
    "            # Attempt to extract per-dimension stats/p-values if exposed\n",
    "            for m in range(2, max_dim + 1):\n",
    "                try:\n",
    "                    stat = res.stat[m]\n",
    "                    pval = res.pvalue[m]\n",
    "\n",
    "                except Exception:\n",
    "                    stat, pval = None, None\n",
    "                out[\"results\"][f\"m_{m}\"] = {\"stat\": _safe_float(stat), \"p\": _safe_float(pval)}\n",
    "\n",
    "        except Exception as e:\n",
    "            out[\"error\"] = str(e)\n",
    "\n",
    "    elif bds_impl == \"sm\":\n",
    "        try:\n",
    "            for m in range(2, max_dim + 1):\n",
    "                try:\n",
    "                    stat, pval = bds_sm(s.values, m=m, eps=None)\n",
    "\n",
    "                except Exception:\n",
    "                    stat, pval = None, None\n",
    "                out[\"results\"][f\"m_{m}\"] = {\"stat\": _safe_float(stat), \"p\": _safe_float(pval)}\n",
    "                \n",
    "        except Exception as e:\n",
    "            out[\"error\"] = str(e)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dcfed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from numpy.linalg import matrix_rank\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "\n",
    "def robust_tsay_style_test(y: pd.Series, p: int = 5):\n",
    "    \"\"\"\n",
    "    Tsay-style nonlinearity check:\n",
    "    AR(p) vs AR(p)+quadratic(lags^2). Handles rank deficiency and provides fallbacks.\n",
    "    Returns dict with 'method' in {'F-nested','RESET','unavailable'}.\n",
    "    \"\"\"\n",
    "    y = pd.Series(y).dropna().astype(float)\n",
    "\n",
    "    # Build lag matrix\n",
    "    X_lin = pd.concat([y.shift(i) for i in range(1, p+1)], axis=1)\n",
    "    X_lin.columns = [f\"L{i}\" for i in range(1, p+1)]\n",
    "    df = pd.concat([y, X_lin], axis=1).dropna()\n",
    "    if df.empty or df.shape[0] < (p + 10):\n",
    "        return {\"method\": \"unavailable\", \"error\": \"not enough data after lags\"}\n",
    "\n",
    "    y0 = df.iloc[:, 0].values\n",
    "    Xl = df.loc[:, X_lin.columns].values\n",
    "\n",
    "    # Standardize lags to mitigate collinearity\n",
    "    Xl = (Xl - Xl.mean(axis=0, keepdims=True)) / (Xl.std(axis=0, keepdims=True) + 1e-12)\n",
    "\n",
    "    # Quadratic terms\n",
    "    Xq = Xl**2\n",
    "\n",
    "    # Drop zero-variance columns\n",
    "    keep_lin = Xl.std(axis=0) > 1e-12\n",
    "    keep_quad = Xq.std(axis=0) > 1e-12\n",
    "    Xl = Xl[:, keep_lin]\n",
    "    Xq = Xq[:, keep_quad]\n",
    "\n",
    "    # If no valid quadratic terms remain, bail out\n",
    "    if Xq.shape[1] == 0:\n",
    "        # Fall back to RESET (general nonlinearity)\n",
    "        X1 = sm.add_constant(Xl, has_constant='add')\n",
    "        m1 = sm.OLS(y0, X1).fit()\n",
    "        try:\n",
    "            reset = linear_reset(m1, power=2, use_f=True)\n",
    "            return {\"method\": \"RESET\", \"F\": float(reset.fvalue), \"p\": float(reset.pvalue)}\n",
    "        except Exception as e:\n",
    "            return {\"method\": \"unavailable\", \"error\": f\"No quadratic terms; RESET failed: {e}\"}\n",
    "\n",
    "    # Remove duplicate columns (within tol) to increase effective rank\n",
    "    def unique_cols(X, tol=1e-10):\n",
    "        cols = []\n",
    "        idx = []\n",
    "        for j in range(X.shape[1]):\n",
    "            x = X[:, [j]]\n",
    "            if not cols:\n",
    "                cols.append(x)\n",
    "                idx.append(j)\n",
    "            else:\n",
    "                M = np.hstack(cols + [x])\n",
    "                if matrix_rank(M) > matrix_rank(np.hstack(cols)) + 0:  # added rank?\n",
    "                    cols.append(x)\n",
    "                    idx.append(j)\n",
    "        return X[:, idx]\n",
    "\n",
    "    Xl_u = unique_cols(Xl)\n",
    "    Xq_u = unique_cols(Xq)\n",
    "\n",
    "    # Design matrices\n",
    "    X1 = sm.add_constant(Xl_u, has_constant='add')\n",
    "    X2 = sm.add_constant(np.hstack([Xl_u, Xq_u]), has_constant='add')\n",
    "\n",
    "    # Check ranks\n",
    "    r1 = matrix_rank(X1)\n",
    "    r2 = matrix_rank(X2)\n",
    "    if r2 <= r1:\n",
    "        # No added rank → cannot form nested F; fall back to RESET\n",
    "        m1 = sm.OLS(y0, X1).fit()\n",
    "        try:\n",
    "            reset = linear_reset(m1, power=2, use_f=True)\n",
    "            return {\"method\": \"RESET\", \"F\": float(reset.fvalue), \"p\": float(reset.pvalue)}\n",
    "        except Exception as e:\n",
    "            return {\"method\": \"unavailable\", \"error\": f\"Rank did not increase; RESET failed: {e}\"}\n",
    "\n",
    "    # Fit and compare nested F\n",
    "    m1 = sm.OLS(y0, X1).fit()\n",
    "    m2 = sm.OLS(y0, X2).fit()\n",
    "    try:\n",
    "        F, pval, df_diff = m2.compare_f_test(m1)  # robust nested F\n",
    "        return {\"method\": \"F-nested\", \"F\": float(F), \"p\": float(pval), \"df_diff\": int(df_diff)}\n",
    "    except Exception as e:\n",
    "        # Final fallback: RESET on the linear model\n",
    "        try:\n",
    "            reset = linear_reset(m1, power=2, use_f=True)\n",
    "            return {\"method\": \"RESET\", \"F\": float(reset.fvalue), \"p\": float(reset.pvalue), \"note\": str(e)}\n",
    "        except Exception as e2:\n",
    "            return {\"method\": \"unavailable\", \"error\": f\"compare_f_test failed: {e}; RESET failed: {e2}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41458bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_mutual_information(s: pd.Series, max_lag: int = 30, n_neighbors: int = 5, random_state: int = 0) -> pd.DataFrame:\n",
    "    y = _series_clean(s)\n",
    "    rows = []\n",
    "\n",
    "    for L in range(1, max_lag + 1):\n",
    "        x = y.shift(L).dropna()\n",
    "        z = y.loc[x.index]\n",
    "\n",
    "        if len(x) < 50:\n",
    "            rows.append((L, np.nan))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            mi = mutual_info_regression(x.values.reshape(-1, 1), z.values,\n",
    "                                        n_neighbors=n_neighbors, random_state=random_state)[0]\n",
    "            \n",
    "        except Exception:\n",
    "            mi = np.nan\n",
    "\n",
    "        rows.append((L, mi))\n",
    "        \n",
    "    return pd.DataFrame(rows, columns=[\"lag\", \"mutual_information\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1b87d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_acf(s: pd.Series, tau: float = 0.05, max_lag: int = 20) -> pd.DataFrame:\n",
    "    y = _series_clean(s)\n",
    "    thr = y.quantile(tau)\n",
    "    I = (y <= thr).astype(int)\n",
    "    rows = []\n",
    "\n",
    "    for L in range(1, max_lag + 1):\n",
    "        v1 = I[L:].values\n",
    "        v2 = I.shift(L)[L:].values\n",
    "\n",
    "        if len(v1) < 10:\n",
    "            rows.append((L, np.nan))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            c = np.corrcoef(v1, v2)[0, 1]\n",
    "\n",
    "        except Exception:\n",
    "            c = np.nan\n",
    "\n",
    "        rows.append((L, c))\n",
    "        \n",
    "    return pd.DataFrame(rows, columns=[\"lag\", \"qacf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5b6782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signs_runs_test(s: pd.Series) -> Dict[str, Any]:\n",
    "    if not RUNS_AVAILABLE:\n",
    "        return {\"available\": False, \"error\": \"runs_test not available\"}\n",
    "    \n",
    "    y = _series_clean(s)\n",
    "    signs = np.sign(y.values)\n",
    "    signs = signs[signs != 0]\n",
    "\n",
    "    if len(signs) < 50:\n",
    "        return {\"available\": True, \"error\": \"not enough non-zero signs\"}\n",
    "    \n",
    "    try:\n",
    "        z_stat, pval = runs_test(signs)\n",
    "        return {\"available\": True, \"z\": _safe_float(z_stat), \"p\": _safe_float(pval)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"available\": True, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37e82363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_switching_fit(s: pd.Series) -> Dict[str, Any]:\n",
    "    if not MS_AVAILABLE:\n",
    "        return {\"available\": False, \"error\": \"MarkovRegression not available\"}\n",
    "    \n",
    "    y = _series_clean(s)\n",
    "\n",
    "    try:\n",
    "        mod = MarkovRegression(y, k_regimes=2, trend=\"c\", switching_variance=True)\n",
    "        res = mod.fit(disp=False, maxiter=200)\n",
    "        out = {\n",
    "            \"available\": True,\n",
    "            \"llf\": _safe_float(res.llf),\n",
    "            \"params\": [ _safe_float(v) for v in res.params.tolist() ],\n",
    "            \"avg_regime_probs\": [ _safe_float(v) for v in res.smoothed_marginal_probabilities.mean(axis=0).tolist() ]\n",
    "        }\n",
    "\n",
    "        return out\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"available\": True, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fadfbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_battery_for_series(s: pd.Series, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a comprehensive set of tests on a single time series.\n",
    "    Returns a nested dict of results.\n",
    "    \"\"\"\n",
    "    cfg = {\n",
    "        \"lb_lags\": [10, 20, 40],\n",
    "        \"arch_lags\": [5, 10, 20],\n",
    "        \"bds_max_dim\": 5,\n",
    "        \"tsay_p\": 5,\n",
    "        \"mi_max_lag\": 20,\n",
    "        \"mi_neighbors\": 5,\n",
    "        \"qacf_tau_list\": [0.01, 0.05, 0.10],\n",
    "        \"qacf_max_lag\": 20,\n",
    "        \"shapiro_max_n\": 5000,\n",
    "        \"with_markov_switching\": True\n",
    "    }\n",
    "\n",
    "    if config:\n",
    "        cfg.update(config)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Descriptives & normality\n",
    "    results[\"summary\"] = summarize_series(s)\n",
    "    results[\"normality\"] = normality_tests(s, shapiro_max_n=cfg[\"shapiro_max_n\"])\n",
    "\n",
    "    # Linear dependence & heteroskedasticity\n",
    "    results[\"autocorr\"] = autocorr_tests(s, lb_lags=cfg[\"lb_lags\"])\n",
    "    results[\"arch_lm\"] = arch_lm_tests(s, lags=cfg[\"arch_lags\"])\n",
    "\n",
    "    # Stationarity\n",
    "    results[\"stationarity\"] = stationarity_tests(s)\n",
    "\n",
    "    # Nonlinear / information tests\n",
    "    results[\"bds\"] = bds_test(s, max_dim=cfg[\"bds_max_dim\"])\n",
    "    results[\"tsay\"] = robust_tsay_style_test(s, p=cfg[\"tsay_p\"])\n",
    "    results[\"runs\"] = signs_runs_test(s)\n",
    "\n",
    "    # Mutual information & quantile ACF (store as dict for JSON-ability)\n",
    "    mi_df = lagged_mutual_information(s, max_lag=cfg[\"mi_max_lag\"], n_neighbors=cfg[\"mi_neighbors\"])\n",
    "    results[\"mutual_information\"] = mi_df.to_dict(orient=\"list\")\n",
    "\n",
    "    qacf_by_tau = {}\n",
    "\n",
    "    for tau in cfg[\"qacf_tau_list\"]:\n",
    "        qacf_df = quantile_acf(s, tau=tau, max_lag=cfg[\"qacf_max_lag\"])\n",
    "        qacf_by_tau[str(tau)] = qacf_df.to_dict(orient=\"list\")\n",
    "        \n",
    "    results[\"qacf\"] = qacf_by_tau\n",
    "\n",
    "    # Optional regime switching\n",
    "    if cfg.get(\"with_markov_switching\", True):\n",
    "        results[\"markov_switching\"] = markov_switching_fit(s)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27915439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_battery_over_stride_dict(stride_dict: Dict[Union[str, int], pd.Series], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply the battery to each horizon (key) in a dict of pandas Series.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    for key, series in stride_dict.items():\n",
    "        try:\n",
    "            out[str(key)] = run_battery_for_series(series, config=config)\n",
    "\n",
    "        except Exception as e:\n",
    "            out[str(key)] = {\"error\": str(e)}\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80bf04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_results_to_dataframe(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten the nested results dict into a DataFrame for quick filtering/comparison.\n",
    "    Each top-level key is a horizon; columns are hierarchical paths.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for horizon, res in results.items():\n",
    "        flat = {}\n",
    "\n",
    "        def _flatten(prefix, obj):\n",
    "            if isinstance(obj, dict):\n",
    "                for k, v in obj.items():\n",
    "                    _flatten(f\"{prefix}.{k}\" if prefix else str(k), v)\n",
    "\n",
    "            elif isinstance(obj, list):\n",
    "                flat[prefix] = str(obj)  # keep as string for overview\n",
    "\n",
    "            else:\n",
    "                flat[prefix] = v if (v := obj) is not None else np.nan\n",
    "\n",
    "        _flatten(\"\", res)\n",
    "        flat[\"horizon\"] = horizon\n",
    "        rows.append(flat)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"horizon\")\n",
    "\n",
    "    # Move a few key p-values (if present) up front for convenience\n",
    "    preferred_cols = [c for c in [\n",
    "        \"normality.jb_p\",\n",
    "        \"autocorr.ljung_box_returns.lags_20\",\n",
    "        \"autocorr.ljung_box_squared.lags_20\",\n",
    "        \"arch_lm.lags_10.LM_p\",\n",
    "        \"stationarity.adf_p\",\n",
    "        \"stationarity.kpss_p\",\n",
    "        \"tsay.p\",\n",
    "        \"runs.p\",\n",
    "        \"bds.results.m_2.p\",\n",
    "        \"bds.results.m_3.p\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    others = [c for c in df.columns if c not in preferred_cols]\n",
    "    \n",
    "    return df[preferred_cols + others]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "809a249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\tsa\\stattools.py:2022: InterpolationWarning: The test statistic is outside of the range of p-values available in the\n",
      "look-up table. The actual p-value is greater than the p-value returned.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\llucp\\AppData\\Roaming\\Python\\Python39\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "results = run_battery_over_stride_dict(\n",
    "    fwd_log_returns_dict, config={\n",
    "        \"lb_lags\": [10, 20, 40],\n",
    "        \"arch_lags\": [5, 10, 20],\n",
    "        \"bds_max_dim\": 5,\n",
    "        \"tsay_p\": 5,\n",
    "        \"mi_max_lag\": 20,\n",
    "        \"mi_neighbors\": 5,\n",
    "        \"qacf_tau_list\": [0.01, 0.05, 0.10],\n",
    "        \"qacf_max_lag\": 20,\n",
    "        \"with_markov_switching\": True,   # set False if needed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "616a7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overview = flatten_results_to_dataframe(results)\n",
    "df_overview.to_csv(\"results_overview.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
